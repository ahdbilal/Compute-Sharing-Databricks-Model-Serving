# Compute-Sharing-Databricks-Model-Serving

This repository demonstrates the deployment of MLflow models by leveraging compute sharing on a model serving endpoint. By sharing compute resources, you can optimize costs and reduce the total cost of ownership for your ML deployments.

## Getting Started
To get started, add [this notebook](Sharing%20Endpoint%20Compute%20Across%20Multiple%20Models.py) to your Databricks workspace. You can also start with [this example]([Example]%20Sharing%20Endpoint%20Compute%20Across%20Multiple%20Models.py) that trains two models and then deploy those to a single endpoint

